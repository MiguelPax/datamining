{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import scala.io.Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "The task is to implement the stages of finding textually similar documents based on Jaccard similarity using the shingling, minhashing, and locality-sensitive hashing (LSH) techniques and corresponding algorithms. \n",
    "\n",
    "![bigpicture](./docs/bigpicture.png/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The sample data are textfiles of hotel reviews. They are loaded into a dataframe, textfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 30 2009 \tOK value though shabby; good position.\tI extended a one night stay to three. The staff were basically helpful when asked. My room was clean, comfortable, and daily serviced. I think the position is good for a budget hotel, near the Golden Gate theatre, right by Asian Arts Museum and Civic Centre. It's an old and architecturally atmospheric hotel.Negatives: it is very badly neglected. Things that might put you off are the need usually to ring the outside doorbell and wait to get someone to the unattended reception, unreliable breakfast availability (nil when I was there but I can see from other reviews that it can appear!), a very unreliable (but rather beautiful) classic Otis elevator, some very dilapidated corridors on the way to your room when the elevator is kaput, shabby though formerly elegant decor, homeless on nearby streets, though the street immediately outside was clear and felt safe and I didn't see any drug taking or pushing and I was never bothered anywhere. However, I rather liked both hotel and neighbourhood and felt it was OK value. One thing - when I extended my stay they raised the rate above the internet rate which was in the fifties to $70.\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "documents = non-empty iterator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "non-empty iterator"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val documents = sc.textFile(\"../data/OpinRankDataset/hotels/small_sample/*\").toDF\n",
    "val documents = Source.fromFile(\"../data/OpinRankDataset/hotels/small_sample/usa_san francisco_abigail_hotel\").getLines()\n",
    "\n",
    "documents.take(1).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Shinglesss\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.util.hashing\n",
    "import scala.math\n",
    "\n",
    "class Shinglesss extends Serializable {\n",
    "    def docToShingles(doc: String, k: Integer) : (Set[Integer]) = {\n",
    "\n",
    "        var shingles : Set[String] = Set()\n",
    "\n",
    "        // number of shingles = length/k\n",
    "        val n = doc.length/k\n",
    "\n",
    "        // for loop slice string\n",
    "        for( i <- 0 to (n-1)){\n",
    "           shingles += doc.slice(k * i, k * (i+1))\n",
    "        }\n",
    "        // hash each shingle in the set\n",
    "        val hashedShingles = shingles.map(x => hash(x))\n",
    "        hashedShingles\n",
    "    }\n",
    "    def hash(s: String) : (Integer) = {\n",
    "      // Hashes a string to one out of 2^32 buckets with mod(2^32)\n",
    "      val n = Math.pow(2, 32)\n",
    "      val intRep = s.getBytes.foldLeft(0)(_+_).asInstanceOf[Integer]\n",
    "      val hash = (intRep % n).asInstanceOf[Integer]\n",
    "      hash\n",
    "    }\n",
    "\n",
    "    def compareSets(s1: Array[Integer], s2: Array[Integer]) : (Double) = {\n",
    "        // comparesets that compares two sets of shingles and returns the similarity\n",
    "        \n",
    "        val union = s1.union(s2)\n",
    "\n",
    "        val intersection = s1.intersect(s2)\n",
    "\n",
    "        val similarity = intersection.size/union.size\n",
    "\n",
    "        (similarity)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "// def Shingling - returns a k-shingle representation of a text document\n",
    "// can either use set or multiset (allowing multi occuring shingles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.ClassCastException\n",
       "Message: java.lang.Double cannot be cast to java.lang.Integer\n",
       "StackTrace:   at Shinglesss.hash(<console>:40)\n",
       "  at Shinglesss$$anonfun$1.apply(<console>:33)\n",
       "  at Shinglesss$$anonfun$1.apply(<console>:33)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:316)\n",
       "  at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972)\n",
       "  at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:47)\n",
       "  at scala.collection.SetLike$class.map(SetLike.scala:92)\n",
       "  at scala.collection.AbstractSet.map(Set.scala:47)\n",
       "  at Shinglesss.docToShingles(<console>:33)\n",
       "  at $anonfun$1.apply(<console>:48)\n",
       "  at $anonfun$1.apply(<console>:48)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at scala.collection.Iterator$$anon$10.next(Iterator.scala:393)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s = new Shinglesss()\n",
    "val k = 10\n",
    "val shingleDocs = documents.map(x => s.docToShingles(x, k))\n",
    "shingleDocs.take(3).foreach(println)\n",
    "// val similarities = shingleDocs.map(x => s.compareSets(x, x)).toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class NewTokenlizer10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NewTokenlizer10 extends Serializable{\n",
    "      val tokens: (String, Integer) => Set[Integer] = (doc:String, k:Integer) => {\n",
    "\n",
    "            var shingles : Set[String] = Set()\n",
    "\n",
    "            // number of shingles = length/k\n",
    "            val n = doc.length/k\n",
    "\n",
    "            // for loop slice string\n",
    "            for( i <- 0 to (n-1)){\n",
    "               shingles += doc.slice(k * i, k * (i+1))\n",
    "            }\n",
    "        // hash each shingle in the set\n",
    "        val hashedShingles = shingles.map(x => hash(x))\n",
    "        hashedShingles\n",
    "      }\n",
    "    val hash: String => Integer = (s: String) => {\n",
    "      // Hashes a string to one out of 2^32 buckets with mod(2^32)\n",
    "      val n = Math.pow(2, 32)\n",
    "      val intRep = s.getBytes.foldLeft(0)(_+_).asInstanceOf[Integer]\n",
    "      val hash = (intRep % n).asInstanceOf[Integer]\n",
    "      hash\n",
    "    }\n",
    "    val compareSets: (Set[Integer], Set[Integer]) => Double = (s1: Set[Integer], s2: Set[Integer]) => {\n",
    "        // comparesets that compares two sets of shingles and returns the similarity\n",
    "        \n",
    "        val union = s1.union(s2)\n",
    "\n",
    "        val intersection = s1.intersect(s2)\n",
    "\n",
    "        val similarity = intersection.size/union.size\n",
    "        println(similarity)\n",
    "        (similarity)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s = NewTokenlizer10@835cc0a\n",
       "k = 10\n",
       "shingleDocs = [value: array<int>]\n",
       "similarities = [value: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: double]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s = new NewTokenlizer10()\n",
    "val k = 10\n",
    "val shingleDocs = documents.map(x => s.tokens(x.getString(0), k))\n",
    "val similarities = shingleDocs.map(x => s.compareSets(x, x)).toDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Task not serializable\n",
       "StackTrace:   at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:345)\n",
       "  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335)\n",
       "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\n",
       "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2299)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:844)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:843)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:843)\n",
       "  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:608)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:294)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2722)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2722)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
       "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n",
       "  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2722)\n",
       "  ... 44 elided\n",
       "Caused by: java.io.NotSerializableException: org.apache.spark.SparkContext\n",
       "Serialization stack:\n",
       "\t- object not serializable (class: org.apache.spark.SparkContext, value: org.apache.spark.SparkContext@6f4c6044)\n",
       "\t- field (class: $iw, name: sc, type: class org.apache.spark.SparkContext)\n",
       "\t- object (class $iw, $iw@70cbbd7f)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@454ba582)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1573de06)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@52e2ed07)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@706b71a2)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@55b63eaf)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@3662bd92)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@543eaf5b)\n",
       "\t- field (class: $line14.$read, name: $iw, type: class $iw)\n",
       "\t- object (class $line14.$read, $line14.$read@75ea2b31)\n",
       "\t- field (class: $iw, name: $line14$read, type: class $line14.$read)\n",
       "\t- object (class $iw, $iw@4de2c3f1)\n",
       "\t- field (class: $iw, name: $outer, type: class $iw)\n",
       "\t- object (class $iw, $iw@150f3803)\n",
       "\t- field (class: $anonfun$1, name: $outer, type: class $iw)\n",
       "\t- object (class $anonfun$1, <function1>)\n",
       "\t- element of array (index: 1)\n",
       "\t- array (class [Ljava.lang.Object;, size 3)\n",
       "\t- field (class: org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10, name: references$1, type: class [Ljava.lang.Object;)\n",
       "\t- object (class org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10, <function2>)\n",
       "  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n",
       "  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
       "  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n",
       "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// def MinHashing - returns the minhash representation of a text document based on a shingling\n",
    "\n",
    "// def CompareSignatures a method compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:52: error: Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.\n",
       "       val shingleDocs = documents.map(x => s.docToShingles(x.getString(0), k))\n",
       "                                      ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Main procedure\n",
    "val s = new Shingles()\n",
    "\n",
    "val k = 10\n",
    "\n",
    "val shingleDocs = documents.map(x => s.docToShingles(x.getString(0), k))\n",
    "\n",
    "// val similarities = shingleDocs.map(x => s.compareSets(x, x))\n",
    "\n",
    "\n",
    "shingleDocs.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scala.util.hashing\n",
    "import scala.math\n",
    "\n",
    "class Shinglesss extends Serializable {\n",
    "    def docToShingles(doc: String, k: Integer) : (Set[Integer]) = {\n",
    "\n",
    "        var shingles : Set[String] = Set()\n",
    "\n",
    "        // number of shingles = length/k\n",
    "        val n = doc.length/k\n",
    "\n",
    "        // for loop slice string\n",
    "        for( i <- 0 to (n-1)){\n",
    "           shingles += doc.slice(k * i, k * (i+1))\n",
    "        }\n",
    "        // hash each shingle in the set\n",
    "        val hashedShingles = shingles.map(x => hash(x))\n",
    "        hashedShingles\n",
    "    }\n",
    "    def hash(s: String) : (Integer) = {\n",
    "      // Hashes a string to one out of 2^32 buckets with mod(2^32)\n",
    "      val n = Math.pow(2, 32)\n",
    "      val intRep = s.getBytes.foldLeft(0)(_+_).asInstanceOf[Integer]\n",
    "      val hash = (intRep % n).asInstanceOf[Integer]\n",
    "      hash\n",
    "    }\n",
    "\n",
    "    def compareSets(s1: Array[Integer], s2: Array[Integer]) : (Double) = {\n",
    "        // comparesets that compares two sets of shingles and returns the similarity\n",
    "        \n",
    "        val union = s1.union(s2)\n",
    "\n",
    "        val intersection = s1.intersect(s2)\n",
    "\n",
    "        val similarity = intersection.size/union.size\n",
    "\n",
    "        (similarity)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "// def Shingling - returns a k-shingle representation of a text document\n",
    "// can either use set or multiset (allowing multi occuring shingles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
